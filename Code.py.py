# -*- coding: utf-8 -*-
"""Git_gpt2_parameter_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lOnwEBMRa8dLYu4lsyoGTgbvR0ZEqEou
"""

!pip install pyTelegramBotAPI
!pip install transformers torch
!pip install transformers[sentencepiece]

import telebot
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
import time
import requests
import torch

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TelegramLLMBot:
    def __init__(self, bot_token, model_name):
        # Telegram Bot Configuration
        self.bot_token = bot_token
        self.bot = telebot.TeleBot(self.bot_token, threaded=False)

        # LLM Model Configuration
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
        except Exception as e:
            logger.error(f"Model loading error: {e}")
            raise

        # Setup message handlers
        self.setup_handlers()

    def setup_handlers(self):
        @self.bot.message_handler(commands=['start', 'help'])
        def send_welcome(message):
            welcome_text = "Welcome! I'm your AI assistant. Ask me anything."
            self.bot.reply_to(message, welcome_text)

        @self.bot.message_handler(func=lambda message: True)
        def handle_message(message):
            try:
                # Generate response using LLM
                response = self.generate_response(message.text)
                self.bot.reply_to(message, response)
            except Exception as e:
                logger.error(f"Message handling error: {e}")
                self.bot.reply_to(message, "Sorry, I'm having trouble processing your message.")

    def generate_response(self, text):
        try:
            # Tokenize input
            inputs = self.tokenizer(text, return_tensors="pt")

            # Create attention mask
            attention_mask = torch.ones_like(inputs["input_ids"])
            attention_mask[inputs["input_ids"] == self.tokenizer.pad_token_id] = 0

            # Print input shape for debugging (optional)
            # print("inputs shape:", inputs["input_ids"].shape)

            # Generate response
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                max_length=200,
                num_return_sequences=1,
                attention_mask=attention_mask,
                repetition_penalty=1.1,
                #do_sample=True,
                #top_k=20,
                #top_p=0.65,
                #temperature=0.5
            )
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            return response
        except Exception as e:
            logger.error(f"Response generation error: {e}")
            return "I couldn't generate a response. Please try again."

    def stop_previous_bot(self):
        """Attempt to stop any existing bot instances"""
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/getUpdates"
            response = requests.get(url)
            if response.status_code == 200:
                logger.info("Successfully checked existing bot status")
        except Exception as e:
            logger.error(f"Error checking bot status: {e}")

    def start_bot(self):
        """Start bot with robust error handling and restart mechanism"""
        self.stop_previous_bot()

        while True:
            try:
                logger.info("Starting bot polling...")
                self.bot.polling(
                    none_stop=True, # Restart polling if it fails
                    interval=1, # Short interval between polling attempts
                    timeout=30  # Timeout for API requests
                )
            except Exception as e:
                logger.error(f"Bot polling error: {e}")
                time.sleep(10) # Wait before retrying

# Configuration
BOT_TOKEN = 'Your-Telegram-Bot-Token' # Replace with your actual bot token
MODEL_NAME = "gpt2" # You can choose a different model

# Initialize and start bot
def main():
    try:
        bot = TelegramLLMBot(BOT_TOKEN, MODEL_NAME)
        bot.start_bot()
    except Exception as e:
        logger.error(f"Bot initialization error: {e}")

if __name__ == "__main__":
    main()